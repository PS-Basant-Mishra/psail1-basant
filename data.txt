
# AI Level 1 Training Project: Retrieval-Augmented Assistant

## Project Overview
This project is a Proof of Concept (POC) for a Retrieval-Augmented Generation (RAG) assistant. The goal is to demonstrate how Large Language Models (LLMs) can be enhanced with external knowledge to provide accurate, context-aware answers.

## Key Technologies
1.  **Large Language Model (LLM)**: We are using the Qwen3-Coder model (480b-cloud parameter variant). This model is optimized for coding and general reasoning tasks.
2.  **Vector Database**: ChromaDB is used to store semantic embeddings of the document chunks. This allows for fast similarity search.
3.  **Framework**: LangChain is the orchestration framework used to connect the data source, the vector store, and the LLM.
4.  **Interface**: A modern web interface built with HTML5, CSS3, and JavaScript, hosted on a Python Flask backend.

## How It Works
1.  **Ingestion**: The system reads this text file (or other uploaded documents).
2.  **Chunking**: The text is split into smaller, manageable pieces to fit into the model's context window.
3.  **Embedding**: Each chunk is converted into a numerical vector representing its meaning.
4.  **Retrieval**: When a user asks a question, the system finds the most relevant chunks from the database.
5.  **Generation**: The LLM receives the user's question along with the retrieved context to generate a precise answer.

## Advantages of RAG
*   **Accuracy**: Reduces hallucinations by grounding the model in specific data.
*   **Up-to-date**: Can answer questions about private or recent data that the base model wasn't trained on.
*   **Cost-Effective**: No need to fine-tune a massive model; just update the vector database.
